## Дистилляция Знаний (Knowledge Distillation)

Мы реализовали процесс дистилляции знаний, который позволяет передать знания из учительской модели в более простую студенческую модель. В нашем контексте, учительская модель - это `vit_model` (предварительно обученная модель Vision Transformer), а студенческая модель - `student_model` (простая модель ResNet-18). Процесс дистилляции знаний включает в себя несколько этапов:

**Учительская Модель (Исходная):**
- Загружается обученная модель (`vit_model`).
- Применяется к набору изображений для получения предсказаний.

**Вычисление "Мягких" Меток:**
- "Мягкие" метки вычисляются для изображений с использованием температуры T (в данном случае, T = 2.0).
- "Мягкие" метки представляют собой вероятностные распределения классов для каждого изображения и имеют большую разницу между вероятностями классов. Это делает их "мягкими" по сравнению с "жесткими" метками (0 или 1).

**Студенческая Модель (Студент):**
- Создается модель "студента" (`student_model`) на основе ResNet-18 с двумя классами (кот и собака).

**Функция Потерь с Учетом Дистилляции Знаний:**
- Определяется функция потерь, которая учитывает как "жесткие" метки (классификация студента), так и "мягкие" метки (предсказания учителя). Функция потерь включает в себя Cross-Entropy Loss и мягкую потерю (Kullback-Leibler Divergence Loss), которые контролируются параметрами `alpha` и `temperature`.

**Обучение Студента:**
- Модель студента (`student_model`) обучается на основе "мягких" меток, вычисленных на этапе 2. Процесс обучения продолжается в течение нескольких эпох.

**Оценка Студента:**
- После обучения студента, он оценивается на том же наборе изображений, что и учительская модель.
- Вычисляется точность (`student_acc`) студента и время обработки изображений студенческой моделью.
