{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VGZX493gSvp",
        "outputId": "40c6cd27-023e-4eba-a131-624a74d0fbb4"
      },
      "id": "_VGZX493gSvp",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ec6kMkWuh4LB"
      },
      "id": "ec6kMkWuh4LB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "fceab068",
      "metadata": {
        "id": "fceab068"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import models\n",
        "from torch.nn.utils import prune\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2f7a09",
      "metadata": {
        "id": "2a2f7a09"
      },
      "outputs": [],
      "source": [
        "path_to_model = \"/content/drive/MyDrive/model_compression/my_model\"\n",
        "\n",
        "processor = AutoFeatureExtractor.from_pretrained(path_to_model)\n",
        "vit_model = AutoModelForImageClassification.from_pretrained(path_to_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "792f3c53",
      "metadata": {
        "id": "792f3c53"
      },
      "outputs": [],
      "source": [
        "def model_use(model, img):\n",
        "    with torch.no_grad():\n",
        "        logits = model(**img).logits\n",
        "\n",
        "    pred_label = logits.argmax(-1).item()\n",
        "\n",
        "    return model.config.id2label[pred_label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "6e6f626d",
      "metadata": {
        "id": "6e6f626d"
      },
      "outputs": [],
      "source": [
        "images_list = os.listdir('/content/drive/MyDrive/model_compression/data')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# num of parameters\n",
        "vit_model.num_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LMtMxaWnCDg",
        "outputId": "0b359e27-2278-4b4b-f12d-de17708eaea4"
      },
      "id": "4LMtMxaWnCDg",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85800194"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# size in Mb\n",
        "param_size = 0\n",
        "for param in vit_model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in vit_model.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model ViT size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QZG67OGnDlt",
        "outputId": "becea6f3-2ffe-400d-a055-79a258766a26"
      },
      "id": "9QZG67OGnDlt",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model ViT size: 327.302MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Labels:\n",
        "\n",
        "{\n",
        "  \"cat\": 0,\n",
        "  \"dog\": 1,\n",
        "}\n"
      ],
      "metadata": {
        "id": "VoStsiJYoT9C"
      },
      "id": "VoStsiJYoT9C"
    },
    {
      "cell_type": "code",
      "source": [
        "images_list[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA5SrPdspLaA",
        "outputId": "74ac73be-3332-43a6-8e26-a0f36cebb2aa"
      },
      "id": "kA5SrPdspLaA",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cat.0.jpg', 'cat.1.jpg', 'cat.10.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "1ba0e8bc",
      "metadata": {
        "id": "1ba0e8bc"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "\n",
        "target_lst = []\n",
        "predict_lst = []\n",
        "\n",
        "for img_name in images_list:\n",
        "    img_path = os.path.join('/content/drive/MyDrive/model_compression/data', img_name)  # Full image path\n",
        "\n",
        "    image = Image.open(img_path, mode='r')\n",
        "\n",
        "    inputs = processor(image, return_tensors=\"pt\")\n",
        "    predicts = model_use(vit_model, inputs)\n",
        "    target = img_name[:img_name.find(\".\")]\n",
        "\n",
        "    if target == \"dog\":\n",
        "        label = 1\n",
        "    else:\n",
        "        label = 0\n",
        "\n",
        "    target_lst.append(label)\n",
        "\n",
        "    if predicts == \"dogs\":\n",
        "        pr = 1\n",
        "    else:\n",
        "        pr = 0\n",
        "\n",
        "    predict_lst.append(pr)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "acc = accuracy_score(target_lst, predict_lst)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"accuracy исходной модели= \", acc)\n",
        "print(\"Время обработки изображений исходной модели= \", end-start, \" секунд\")\n",
        "print(\"Скорость обработки изображений у исходной модели составила  \", len(images_list)/(end-start), \" картинок в секунду\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4SQGf7XsNdN",
        "outputId": "fad73aeb-6117-4f20-c65a-a50e65b5d844"
      },
      "id": "E4SQGf7XsNdN",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy исходной модели=  0.9875\n",
            "Время обработки изображений исходной модели=  83.3334059715271  секунд\n",
            "Скорость обработки изображений у исходной модели составила   1.9199983264174745  картинок в секунду\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "3e427d9f",
      "metadata": {
        "id": "3e427d9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ab3a78-8316-4a5c-cf46-7cfa4ea704fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg inference time: 527.7798 ms\n"
          ]
        }
      ],
      "source": [
        "infer_time = ((end - start) / len(images_list)) * 1000\n",
        "print(f'Avg inference time: {infer_time:.4f} ms')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Квантизация:**\n",
        "\n",
        "Dynamic quantization is a method of quantization where the weights and activations are quantized at runtime"
      ],
      "metadata": {
        "id": "14zkEVTfx8mt"
      },
      "id": "14zkEVTfx8mt"
    },
    {
      "cell_type": "code",
      "source": [
        "vit_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU7LYQuNyGdA",
        "outputId": "d3bf0e14-1587-4068-f37b-e188ea7f5728"
      },
      "id": "vU7LYQuNyGdA",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTForImageClassification(\n",
              "  (vit): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTAttention(\n",
              "            (attention): ViTSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PTSQ - Post Training Static Quantization\n",
        "\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    vit_model,\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n"
      ],
      "metadata": {
        "id": "1oEyl46zyt6u"
      },
      "id": "1oEyl46zyt6u",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_size = 0\n",
        "for param in quantized_model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in quantized_model.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('quantized_model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A44fCtqI1Fs1",
        "outputId": "6a8f9768-6299-448d-9236-5f31a679bbe8"
      },
      "id": "A44fCtqI1Fs1",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quantized_model size: 2.979MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "target_lst = []\n",
        "predict_lst = []\n",
        "\n",
        "for img_name in images_list:\n",
        "    img_path = os.path.join('/content/drive/MyDrive/model_compression/data', img_name)  # Full image path\n",
        "\n",
        "    image = Image.open(img_path, mode='r')\n",
        "\n",
        "    inputs = processor(image, return_tensors=\"pt\")\n",
        "    predicts = model_use(quantized_model, inputs)\n",
        "    target = img_name[:img_name.find(\".\")]\n",
        "\n",
        "    label = 1 if img_name.startswith(\"dog\") else 0\n",
        "    target_lst.append(label)\n",
        "\n",
        "    pr = 1 if predicts == \"dogs\" else 0\n",
        "    predict_lst.append(pr)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print('quantized_model:\\n')\n",
        "print(\"accuracy исходной модели= \", acc)\n",
        "print(\"Время обработки изображений исходной модели= \", end-start, \" секунд\")\n",
        "print(\"Скорость обработки изображений у исходной модели составила  \", len(images_list)/(end-start), \" картинок в секунду\")\n",
        "print(f'Avg inference time : {infer_time:.4f} ms')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT6OJkjJ1-fW",
        "outputId": "09846b77-716e-4913-947c-9217db74e788"
      },
      "id": "bT6OJkjJ1-fW",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quantized_model:\n",
            "\n",
            "accuracy исходной модели=  0.975\n",
            "Время обработки изображений исходной модели=  54.9720938205719  секунд\n",
            "Скорость обработки изображений у исходной модели составила   2.9105676877114712  картинок в секунду\n",
            "Avg inference time : 360.0799 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Прунинг"
      ],
      "metadata": {
        "id": "sV_5iUHQ62-f"
      },
      "id": "sV_5iUHQ62-f"
    },
    {
      "cell_type": "code",
      "source": [
        "vit_model.modules()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqLw8SGw8TA9",
        "outputId": "ecc3e9ed-9b2a-4f24-f432-5b0b69277baf"
      },
      "id": "uqLw8SGw8TA9",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.modules at 0x7b2514aa5cb0>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "vit_model_copy = copy.deepcopy(vit_model)"
      ],
      "metadata": {
        "id": "YqBew03cAmwP"
      },
      "id": "YqBew03cAmwP",
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "pruning_ratio = 0.5\n",
        "\n",
        "parameters_to_prune = []\n",
        "\n",
        "# prune the classifier layer\n",
        "parameters_to_prune.append((vit_model_copy.classifier, 'weight'))\n",
        "\n",
        "pruning_method = torch.nn.utils.prune.L1Unstructured\n",
        "\n",
        "for layer, parameter_name in parameters_to_prune:\n",
        "    prune_amount = int(layer.weight.numel() * pruning_ratio)\n",
        "    pruning_method.apply(layer, parameter_name, amount=prune_amount)\n",
        "\n",
        "# Remove re-parametrization after pruning\n",
        "for layer, parameter_name in parameters_to_prune:\n",
        "    prune.remove(layer, 'weight')\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "target_lst = []\n",
        "predict_lst = []\n",
        "\n",
        "for img_name in images_list:\n",
        "    img_path = os.path.join('/content/drive/MyDrive/model_compression/data', img_name)  # Full image path\n",
        "\n",
        "    image = Image.open(img_path, mode='r')\n",
        "\n",
        "    inputs = processor(image, return_tensors=\"pt\")\n",
        "    predicts = model_use(vit_model_copy, inputs)\n",
        "    target = img_name[:img_name.find(\".\")]\n",
        "\n",
        "    label = 1 if img_name.startswith(\"dog\") else 0\n",
        "    target_lst.append(label)\n",
        "\n",
        "    pr = 1 if predicts == \"dogs\" else 0\n",
        "    predict_lst.append(pr)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print('pruned_model:\\n')\n",
        "print(\"accuracy = \", acc)\n",
        "print(\"Время обработки изображений модели= \", end-start, \" секунд\")\n",
        "print(\"Скорость обработки изображений модели составила  \", len(images_list)/(end-start), \" картинок в секунду\")\n",
        "print(f'Avg inference time : {infer_time:.4f} ms')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPPWhsya7GF4",
        "outputId": "d253f009-f07b-47fb-abb8-6e7c2766e8b9"
      },
      "id": "OPPWhsya7GF4",
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pruned_model:\n",
            "\n",
            "accuracy =  0.975\n",
            "Время обработки изображений модели=  87.96074986457825  секунд\n",
            "Скорость обработки изображений модели составила   1.8189931332592235  картинок в секунду\n",
            "Avg inference time : 360.0799 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eeKVZpNG7V-g"
      },
      "id": "eeKVZpNG7V-g",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}